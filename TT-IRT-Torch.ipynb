{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import lr_scheduler\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "# Aux basis functions\n",
    "def interp1(grid, x, f, axis=0):  # Linear interpolation\n",
    "    s = torch.ones(f.dim(), dtype=torch.long)\n",
    "    s[axis] = x.numel()\n",
    "    s = torch.Size(s)\n",
    "    ind = torch.searchsorted(grid, x) # indices on the right of x\n",
    "    ind = torch.minimum(torch.maximum(ind, torch.tensor(1, device=ind.device)), torch.tensor(grid.numel()-1, device=ind.device))\n",
    "    gr = grid[ind].reshape(s)\n",
    "    gl = grid[ind-1].reshape(s)\n",
    "    fr = torch.index_select(f,axis,ind)\n",
    "    fl = torch.index_select(f,axis,ind-1)\n",
    "    xt = x.reshape(s)\n",
    "    return (fr * (xt - gl) + fl * (gr - xt)) / (gr - gl)\n",
    "\n",
    "def linearMass(grid):   # Mass matrix of piecewise linear functions\n",
    "    h = grid[1:] - grid[:-1]\n",
    "    h = torch.cat([h[0].reshape(1), h, h[-1].reshape(1)])\n",
    "    M = torch.diag(h[1:-1]/6, -1) + torch.diag(h[1:-1]/6, 1) + torch.diag((h[:-1]+h[1:])/3)\n",
    "    M[0,0] = h[1]/3 \n",
    "    M[-1,-1] = h[-1]/3\n",
    "    return M\n",
    "\n",
    "def linearLap(grid):    # Laplacian matrix of piecewise linear functions\n",
    "    h = grid[1:] - grid[:-1]\n",
    "    h = torch.cat([h[0].reshape(1), h, h[-1].reshape(1)])\n",
    "    M = torch.diag(-1/h[1:-1], -1) + torch.diag(-1/h[1:-1], 1) + torch.diag(1/h[:-1]+1/h[1:])\n",
    "    M[0,0] = 1/h[1]\n",
    "    M[-1,-1] = 1/h[-1]\n",
    "    return M\n",
    "\n",
    "# Aux cross functions\n",
    "def maxvol2(A, niters=100, eps=1e-2):\n",
    "    \"\"\"\n",
    "    Maxvol algorithm for selecting the most important rows of a matrix\n",
    "    \"\"\"\n",
    "    LU, piv, _ = torch.linalg.lu_factor_ex(A)   \n",
    "    ind = torch.arange(A.shape[0], device=A.device)\n",
    "    for i,p in enumerate(piv-1): # Swap rows\n",
    "        ind[i], ind[p] = ind[p]+0, ind[i]+0\n",
    "    ind = ind.index_select(0, torch.arange(A.shape[1], device=A.device))\n",
    "    B = torch.linalg.solve(A.index_select(0, ind), A, left=False)\n",
    "    for _ in range(niters):\n",
    "        big_ind = torch.argmax(torch.abs(B))\n",
    "        i0, j0 = torch.unravel_index(big_ind, A.shape)\n",
    "        Bi = B.index_select(0, i0)\n",
    "        Bj = B.index_select(1, j0)\n",
    "        Bij = Bi.index_select(1, j0)\n",
    "        if torch.abs(Bij) <= 1 + eps:\n",
    "            break\n",
    "        k = ind.index_select(0, j0)\n",
    "        Bk = B.index_select(0, k)\n",
    "        B += Bj * ((Bk - Bi) / Bij)\n",
    "        ind[j0] = i0        \n",
    "    return ind\n",
    "\n",
    "def qrmaxvol(yc, yn, dir, z=None):\n",
    "    \"\"\"\n",
    "    Move non-orth center between the TT cores, computing enrich, QR and maxvol\n",
    "    \"\"\"\n",
    "    r0, n, r1 = yc.shape\n",
    "    if dir > 0:\n",
    "        yc = yc.reshape(r0*n, r1)\n",
    "    else:\n",
    "        yc = yc.reshape(r0, n*r1).T\n",
    "    if z is not None:\n",
    "        yc = torch.hstack([yc, z])\n",
    "\n",
    "    yq, rv = torch.linalg.qr(yc, mode='reduced')\n",
    "    rv = rv.index_select(1, torch.arange(yc.shape[1], device=yc.device))\n",
    "    ind = maxvol2(yq)\n",
    "    YY = yq.index_select(0, ind)\n",
    "    yq = torch.linalg.solve(YY, yq, left=False)\n",
    "    rv = YY @ rv\n",
    "\n",
    "    if yn is not None:\n",
    "        rnrb = list(yn.shape)\n",
    "        if dir > 0:\n",
    "            yn = yn.reshape(rnrb[0], -1)\n",
    "            yn = rv @ yn\n",
    "            rnrb[0] = rv.shape[0]\n",
    "        else:\n",
    "            yn = yn.reshape(-1, rnrb[-1])\n",
    "            yn = yn @ rv.T\n",
    "            rnrb[-1] = rv.shape[0]\n",
    "        yn = yn.reshape(rnrb)\n",
    "\n",
    "    if dir > 0:\n",
    "        yc = yq.reshape(r0, n, -1)\n",
    "    else:\n",
    "        yc = (yq.T).reshape(-1, n, r1)\n",
    "    return yc, yn, ind\n",
    "\n",
    "def indexmerge(X1, X2, X3=None):\n",
    "    \"\"\"\n",
    "    Merge indices with direct sum\n",
    "    \"\"\"\n",
    "    Y = X2.reshape(1, X2.shape[0], 1, X2.shape[1])\n",
    "    if X1 is not None:\n",
    "        X1 = X1.reshape(X1.shape[0], 1, 1, X1.shape[1])\n",
    "        X1 = torch.tile(X1, [1, Y.shape[1], 1, 1])\n",
    "        Y = torch.tile(Y, [X1.shape[0], 1, 1, 1])\n",
    "        Y = torch.cat([X1, Y], dim=3)\n",
    "    if X3 is not None:\n",
    "        X3 = X3.reshape(1, 1, X3.shape[0], X3.shape[1])\n",
    "        X3 = torch.tile(X3, [Y.shape[0], Y.shape[1], 1, 1])\n",
    "        Y = torch.tile(Y, [1, 1, X3.shape[2], 1])\n",
    "        Y = torch.cat([Y, X3], dim=3)\n",
    "    return Y.reshape(-1, Y.shape[-1])\n",
    "\n",
    "# def ind2samples(grid, ind):\n",
    "#     \"\"\"\n",
    "#     Convert indices to samples on the grid\n",
    "#     \"\"\"\n",
    "#     return torch.hstack([grid[i][indi].reshape(-1,1) for i, indi in enumerate(ind.T)])\n",
    "\n",
    "# def vectorize_fun(fun, X, vectorized=True):\n",
    "#     \"\"\"\n",
    "#     Vectorize function evaluation\n",
    "#     \"\"\"\n",
    "#     if vectorized:\n",
    "#         return fun(X)\n",
    "#     else:\n",
    "#         Y = fun(X[0,:])\n",
    "#         Y = torch.tile(Y, [X.shape[0], 1])\n",
    "#         for i in range(1, X.shape[0]):\n",
    "#             Y[i,:] = fun(X[i,:])\n",
    "#         return Y\n",
    "\n",
    "def evaluate_core(fun, Jl, ii, Jr, Ui):\n",
    "    \"\"\"\n",
    "    Evaluate function for one TT core\n",
    "    \"\"\"\n",
    "    # X = ind2samples(grid, indexmerge(Jl, ii, Jr))\n",
    "    X = indexmerge(Jl, ii, Jr)\n",
    "    Y = fun(X)\n",
    "    # if dir < 0:\n",
    "        # Y = Y.T\n",
    "    Y = Y.reshape(Ui.shape)\n",
    "    return Y, X.shape[0], torch.norm(Y - Ui)/torch.norm(Y)\n",
    "\n",
    "\n",
    "def start_stop_pos(d, dir):\n",
    "    \"\"\"\n",
    "    Start, end and next dimensions for the cross\n",
    "    \"\"\"\n",
    "    if dir > 0:\n",
    "        return 0, d-1, 1\n",
    "    else:\n",
    "        return d-1, 0, 0\n",
    "\n",
    "\n",
    "# Aux MCMC functions\n",
    "def hellinger(lFex, lFapp):\n",
    "    \"\"\"\n",
    "    Hellinger distance between two densities\n",
    "    lFex: log(exact density Fex), can be unnormalised\n",
    "    lFapp: log(approximate density Fapp), must be normalised\n",
    "    \"\"\"\n",
    "    dF = lFex - lFapp\n",
    "    dF = dF - torch.max(dF)\n",
    "    lZex = torch.log(torch.mean(torch.exp(dF))) # up to  +max(dF) which cancels below anyway\n",
    "    H = torch.mean( (torch.exp(0.5*(dF-lZex)) - 1)**2 )\n",
    "    return torch.sqrt(H/2)\n",
    "\n",
    "def mcmc_prune(z, lFex, lFapp):\n",
    "    num_of_rejects = 0\n",
    "    for i in range(lFapp.numel()-1):\n",
    "        alpha = lFex[i+1] - lFapp[i+1] - lFex[i] + lFapp[i]\n",
    "        if torch.exp(alpha) < torch.rand(1, device=lFex.device, dtype=lFex.dtype):\n",
    "            # Reject\n",
    "            z[i+1] = z[i].clone()\n",
    "            lFapp[i+1] = lFapp[i].clone()\n",
    "            lFex[i+1] = lFex[i].clone()\n",
    "            num_of_rejects += 1\n",
    "    print(f\"mcmc_prune completed with {num_of_rejects} rejections ({num_of_rejects*100.0/lFapp.numel()}%)\")\n",
    "    return z, lFex, lFapp, num_of_rejects\n",
    "\n",
    "def randref(reference_sigma, shape, device='cpu', dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Generate random samples from a truncated normal or uniform distribution\n",
    "    \"\"\"\n",
    "    if reference_sigma == 0:\n",
    "        return torch.rand(shape, device=device, dtype=dtype)\n",
    "    else:\n",
    "        cdf_ifactor = torch.erf(reference_sigma/torch.sqrt(torch.tensor(2.0, dtype=dtype))) / 0.5\n",
    "        return torch.erfinv((torch.rand(shape, device=device, dtype=dtype)-0.5)*cdf_ifactor)*torch.sqrt(torch.tensor(2.0, dtype=dtype))\n",
    "    \n",
    "\n",
    "# Classes\n",
    "class TT(nn.Module): # Discrete TT(i) = <U, e_i> for general purpose with MSE loss\n",
    "    def __init__(self, n=[1], r=[1,1], device='cpu', requires_grad=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        if requires_grad:\n",
    "            self.U = nn.ParameterList([nn.Parameter(torch.rand(r[i], n[i], r[i + 1], device=device, dtype=dtype)*1e-3) for i in range(len(n))])\n",
    "        else:\n",
    "            self.U = [torch.rand(r[i], n[i], r[i + 1], device=device, dtype=dtype)*1e-3 for i in range(len(n))]\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        return torch.mean((self.forward(X) - Y)**2)        \n",
    "    \n",
    "\n",
    "class TTLin(TT): # Linear TT(x) = <U, \\phi(x)> for general purpose with MSE loss\n",
    "    def __init__(self, grid=[torch.ones(1)], r=[1,1], requires_grad=True):\n",
    "        super().__init__(n=[g.numel() for g in grid], r=r, device=grid[0].device, requires_grad=requires_grad, dtype=grid[0].dtype)\n",
    "        self.grid = grid\n",
    "\n",
    "    def interp_core(self, k, x):\n",
    "        return interp1(self.grid[k], x, self.U[k], axis=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        prod = self.interp_core(0, X[:,0])\n",
    "        for k in range(1, len(self.U)):\n",
    "            prod = torch.einsum('ijk,kjl->ijl', prod, self.interp_core(k, X[:,k]))\n",
    "        return prod\n",
    "    \n",
    "    def cross(self, fun, tol=1e-6, nswp=20, dir=1):\n",
    "        \"\"\"\n",
    "        Fixed-rank cross approximation\n",
    "        fun should take samples of shape (M, d) and return a tensor of shape (M, m)\n",
    "        \"\"\"\n",
    "        d = len(self.U)\n",
    "        # Initial QR - goes in the reverse of dir\n",
    "        istart, iend, inext = start_stop_pos(d, -dir) # Where we start and move next\n",
    "        J = [None]*(d+1)\n",
    "        for i in range(istart, iend, -dir):\n",
    "            self.U[i], self.U[i-dir], ind = qrmaxvol(self.U[i], self.U[i-dir], -dir)\n",
    "            # Restrict new indices\n",
    "            if -dir > 0:\n",
    "                J[i+inext] = indexmerge(J[i], self.grid[i].reshape(-1,1), None)\n",
    "            else:\n",
    "                J[i+inext] = indexmerge(None, self.grid[i].reshape(-1,1), J[i+1])\n",
    "            J[i+inext] = J[i+inext].index_select(0, ind) # [ind, :]\n",
    "        # # Initial indices\n",
    "        # if (dir>0):\n",
    "        #     J = [None]+[torch.randint(low=0, high=self.U[i].shape[1], size=(self.U[i].shape[0],len(self.U)-i), device=self.U[0].device) for i in range(1,len(self.U))]+[None]\n",
    "\n",
    "        evalcnt = 0\n",
    "        for swp in range(nswp):\n",
    "            istart, iend, inext = start_stop_pos(d, dir) # Where we start and move next\n",
    "            max_err = 0            \n",
    "            for i in range(istart, iend, dir):\n",
    "                # Sample\n",
    "                self.U[i], devalcnt, err = evaluate_core(fun, J[i], self.grid[i].reshape(-1,1), J[i+1], self.U[i])\n",
    "                evalcnt += devalcnt\n",
    "                max_err = max(max_err, err)\n",
    "                print(f\"\\t swp = {swp}.{i}: err = {err:.3e}, #evals = {devalcnt}\")\n",
    "                # Move non-orth center\n",
    "                self.U[i], self.U[i+dir], ind = qrmaxvol(self.U[i], self.U[i+dir], dir)\n",
    "                # Restrict new indices\n",
    "                if dir > 0:\n",
    "                    J[i+inext] = indexmerge(J[i], self.grid[i].reshape(-1,1), None)\n",
    "                else:\n",
    "                    J[i+inext] = indexmerge(None, self.grid[i].reshape(-1,1), J[i+1])\n",
    "                J[i+inext] = J[i+inext].index_select(0, ind) # [ind, :]\n",
    "\n",
    "            print(f\"Cross sweep {swp}: max_err = {max_err:.3e}, cum#evals = {evalcnt}\")\n",
    "            if max_err < tol:\n",
    "                break\n",
    "            if nswp==1: # Last core needs to be computed\n",
    "                i = i+dir\n",
    "                self.U[i], devalcnt, err = evaluate_core(fun, J[i], self.grid[i].reshape(-1,1), J[i+1], self.U[i])\n",
    "                evalcnt += devalcnt\n",
    "                print(f\"\\t swp = {swp}.{i}: err = {err:.3e}, #evals = {devalcnt}\")\n",
    "            dir = -dir\n",
    "        return evalcnt       \n",
    "     \n",
    "    \n",
    "class TTSqrDE(TTLin): # Squared TT(x) = <U, \\phi(x)>^2 for Density Estimation with log-likelihood loss and normalisation\n",
    "    def __init__(self, grid=[torch.ones(1)], r=[1,1], Lambda=0, reference_sigma=0, requires_grad=True):\n",
    "        super().__init__(grid, r, requires_grad=requires_grad)\n",
    "        self.Mass = [linearMass(g) for g in grid]\n",
    "        self.Lambda = Lambda\n",
    "        if Lambda > 0:\n",
    "            self.Lap = [[]]*len(grid)\n",
    "            self.Lap[0] = torch.zeros(1, grid[0].numel(), grid[0].numel(), 2, device=self.U[0].device, dtype=self.U[0].dtype)\n",
    "            self.Lap[0][0,:,:,0] = linearLap(grid[0])\n",
    "            self.Lap[0][0,:,:,1] = self.Mass[0]\n",
    "            for k in range(1,len(grid)):\n",
    "                self.Lap[k] = torch.zeros(2, grid[k].numel(), grid[k].numel(), 2, device=self.U[0].device, dtype=self.U[0].dtype)\n",
    "                self.Lap[k][1,:,:,0] = linearLap(grid[k])\n",
    "                self.Lap[k][0,:,:,0] = self.Mass[k]\n",
    "                self.Lap[k][1,:,:,1] = self.Mass[k]\n",
    "            self.Lap[-1] = torch.zeros(2, grid[-1].numel(), grid[-1].numel(), 1, device=self.U[0].device, dtype=self.U[0].dtype)\n",
    "            self.Lap[-1][0,:,:,0] = self.Mass[-1]\n",
    "            self.Lap[-1][1,:,:,0] = linearLap(grid[-1])\n",
    "        self.reference_sigma = reference_sigma\n",
    "        if reference_sigma > 0:\n",
    "            self.cdf_factor = 0.5/torch.erf(reference_sigma/torch.sqrt(torch.tensor(2.0, dtype=self.U[0].dtype)))\n",
    "        self.marginals = []\n",
    "\n",
    "    def forward(self, X):        \n",
    "        return super().forward(X)**2\n",
    "        \n",
    "    def normconst(self):\n",
    "        self.marginals = [torch.ones(1,1, device=self.U[0].device, dtype=self.U[0].dtype)] * (len(self.U)+1)\n",
    "        for k in range(len(self.U)-1,-1,-1):\n",
    "            self.marginals[k] = torch.einsum('lt,pil,ij,kjt->pk', self.marginals[k+1], self.U[k], self.Mass[k], self.U[k])\n",
    "        return self.marginals[0].flatten()\n",
    "    \n",
    "    def lap(self):\n",
    "        if self.Lambda == 0:\n",
    "            return 0\n",
    "        sm = torch.ones(1,1,1, device=self.U[0].device, dtype=self.U[0].dtype)\n",
    "        for k in range(len(self.U)):\n",
    "            sm = torch.einsum('pqk,pil,qijr,kjt->lrt', sm, self.U[k], self.Lap[k], self.U[k])\n",
    "        return sm.flatten() * self.Lambda\n",
    "\n",
    "    def normalise(self):\n",
    "        Z = self.normconst()\n",
    "        if ((Z-1).abs() > 1e-2):\n",
    "            print(f\"Normalising constant {float(Z)} is far from 1\")\n",
    "        Z = Z**(0.5/len(self.U))\n",
    "        for k in range(len(self.U)):\n",
    "            self.U[k].data /= Z\n",
    "    \n",
    "    def loss(self, X):  # Log-likelihood Loss\n",
    "        return torch.mean(torch.log(self.normconst()) - torch.log(self.forward(X))) + self.lap()  # From TERM\n",
    "        # return -torch.mean(torch.log(self.forward(X)))  # From TERM assuming normalisation\n",
    "    \n",
    "    def rt(self, x):\n",
    "        \"\"\"\n",
    "        Forward Rosenblatt transform\n",
    "        \"\"\"\n",
    "        self.normconst() # Compute marginals\n",
    "        prevcond = torch.ones(q.shape[0],1, device=self.U[0].device, dtype=self.U[0].dtype) # Conditioned left cores\n",
    "        q = torch.ones_like(x)\n",
    "        for k in range(len(self.U)):     \n",
    "            Pprev = torch.einsum('mk,kil->mil', prevcond, self.U[k])\n",
    "            D, C = torch.linalg.eigh(self.marginals[k+1])\n",
    "            D[D<0] = 0\n",
    "            D = torch.sqrt(D)\n",
    "            C = C @ torch.diag(D)\n",
    "            # C, info = torch.linalg.cholesky_ex(self.marginals[k+1], upper=False)\n",
    "            dP = Pprev[:,1:,:] - Pprev[:,:-1,:]\n",
    "            Pprev = Pprev[:,:-1,:]\n",
    "            dP = torch.einsum('mik,kp->mip', dP, C)\n",
    "            Pprev = torch.einsum('mik,kp->mip', Pprev, C)\n",
    "            P0 = Pprev**2\n",
    "            P0 = P0.sum(dim=2)\n",
    "            # print(\"p0 err = \", (torch.einsum('mik,mip,kp->mi', Pprev, Pprev, self.marginals[k+1]) - P0).norm() / P0.norm())\n",
    "            P1 = Pprev*dP*2\n",
    "            P1 = P1.sum(dim=2)\n",
    "            # print(\"p1 err = \", (torch.einsum('mik,mip,kp->mi', dP, Pprev, self.marginals[k+1]) * 2 - P1).norm() / P1.norm())\n",
    "            P2 = dP**2\n",
    "            P2 = P2.sum(dim=2)\n",
    "            # print(\"p2 err = \", (torch.einsum('mik,mip,kp->mi', dP, dP, self.marginals[k+1]) - P2).norm() / P2.norm())\n",
    "            C0h = torch.zeros(P0.shape[0], P0.shape[1]+1, device=self.U[0].device, dtype=self.U[0].dtype) # CDF on grid\n",
    "            h = (self.grid[k][1:] - self.grid[k][:-1]).reshape(1,-1)\n",
    "            C0h[:, 1:] = (P2/3 + P1/2 + P0) * h\n",
    "            C0h = torch.cumsum(C0h, dim=1)  # CDF on grid\n",
    "            Zc  = C0h[:,-1].reshape(-1,1)\n",
    "            i0 = (Zc<1e-30).flatten()\n",
    "            P2[i0,:] = 0\n",
    "            P1[i0,:] = 0\n",
    "            P0[i0,:] = 1\n",
    "            C0h[i0, 1:] = h\n",
    "            C0h[i0, :] = torch.cumsum(C0h[i0, :], dim=1)  # CDF on grid\n",
    "            Zc  = C0h[:,-1].reshape(-1,1)\n",
    "            C0h = C0h / Zc  # Normalise CDF\n",
    "            ind = torch.searchsorted(self.grid[k], x[:,k]) - 1 # indices on the left of x\n",
    "            ind = torch.minimum(torch.maximum(ind, torch.tensor(0, device=self.U[0].device)), torch.tensor(self.grid[k].numel()-2, device=self.U[0].device))\n",
    "            ind = ind.reshape(-1,1)\n",
    "            # C0(z) = (p2*z**3/3 + p1*z**2/2 + p0*z) * h + C0\n",
    "            P0 = torch.gather(P0, dim=1, index=ind) / Zc\n",
    "            P1 = torch.gather(P1, dim=1, index=ind) / Zc\n",
    "            P2 = torch.gather(P2, dim=1, index=ind) / Zc\n",
    "            C0h = torch.gather(C0h, dim=1, index=ind)\n",
    "            xl = self.grid[k][ind].reshape(-1,1)\n",
    "            h = self.grid[k][ind+1].reshape(-1,1) - xl\n",
    "            z = (x[:,k].reshape(-1,1) - xl) / h\n",
    "            q[:,k] = (C0h + (P2 * z**3/3 + P1 * z**2/2 + P0 * z) * h).flatten()\n",
    "            # q[:,k] = torch.maximum(torch.minimum(q[:,k], torch.ones_like(q[:,k])), torch.zeros_like(q[:,k]))\n",
    "            prevcond = torch.einsum('mk,kml->ml', prevcond, self.interp_core(k, x[:,k]))\n",
    "            if self.reference_sigma > 0:\n",
    "                # Map back to truncated normal\n",
    "                q[:,k] = torch.erfinv((q[:,k]-0.5)/self.cdf_factor)*torch.sqrt(torch.tensor(2.0, dtype=self.U[0].dtype))\n",
    "        return q\n",
    "\n",
    "\n",
    "    def irt(self, qn):\n",
    "        \"\"\"\n",
    "        Inverse Rosenblatt transform, up to machine precision (solves cubic equation for CDF inversion)\n",
    "        \"\"\"\n",
    "        q = qn\n",
    "        if self.reference_sigma > 0:\n",
    "            # Map from normal reference to uniform\n",
    "            q = torch.erf(qn/torch.sqrt(torch.tensor(2.0, dtype=self.U[0].dtype))) * self.cdf_factor + 0.5  \n",
    "        self.normconst() # Compute marginals\n",
    "        prevcond = torch.ones(q.shape[0],1, device=self.U[0].device, dtype=self.U[0].dtype) # Conditioned left cores\n",
    "        lFapp = torch.zeros_like(prevcond)\n",
    "        x = torch.ones_like(q)        \n",
    "        for k in range(len(self.U)):\n",
    "            Pprev = torch.einsum('mk,kil->mil', prevcond, self.U[k])\n",
    "            D, C = torch.linalg.eigh(self.marginals[k+1])\n",
    "            D[D<0] = 0\n",
    "            D = torch.sqrt(D)\n",
    "            C = C @ torch.diag(D)\n",
    "            # C, info = torch.linalg.cholesky_ex(self.marginals[k+1], upper=False)\n",
    "            dP = Pprev[:,1:,:] - Pprev[:,:-1,:]\n",
    "            Pprev = Pprev[:,:-1,:]\n",
    "            dP = torch.einsum('mik,kp->mip', dP, C)\n",
    "            Pprev = torch.einsum('mik,kp->mip', Pprev, C)\n",
    "            P0 = Pprev**2\n",
    "            P0 = P0.sum(dim=2)\n",
    "            # print(\"p0 err = \", (torch.einsum('mik,mip,kp->mi', Pprev, Pprev, self.marginals[k+1]) - P0).norm() / P0.norm())\n",
    "            P1 = Pprev*dP*2\n",
    "            P1 = P1.sum(dim=2)\n",
    "            # print(\"p1 err = \", (torch.einsum('mik,mip,kp->mi', dP, Pprev, self.marginals[k+1]) * 2 - P1).norm() / P1.norm())\n",
    "            P2 = dP**2\n",
    "            P2 = P2.sum(dim=2)\n",
    "            # print(\"p2 err = \", (torch.einsum('mik,mip,kp->mi', dP, dP, self.marginals[k+1]) - P2).norm() / P2.norm())\n",
    "            C0h = torch.zeros(P0.shape[0], P0.shape[1]+1, device=self.U[0].device, dtype=self.U[0].dtype) # CDF on grid\n",
    "            h = (self.grid[k][1:] - self.grid[k][:-1]).reshape(1,-1)\n",
    "            # h = xr - xl, dp = pr - pl\n",
    "            # z = (x-xl)/h\n",
    "            # p(z) = (pr_k*z + pl_k*(1-z))*P_{kl}*(pr_l*z + pl_l*(1-z))\n",
    "            #      = (prPpr)*z**2 + (prPpl+plPpr)*z*(1-z) + plPpl*(1-z)**2\n",
    "            #      = (prPpr+plPpl-prPpl-plPpr)*z**2 + (prPpl+plPpr-2*plPpl)*z + plPpl\n",
    "            # p0 = plPpl, p1 = prPpl+plPpr-2*p0 = (pr-pl)Ppl + plP(pr-pl), p2 = prPpr-p1-p0 = (pr-pl)P(pr-pl)\n",
    "            # C0(z) = int_0^z p(s)ds = p2*z**3/3 + p1*z**2/2 + p0*z               * h\n",
    "            # C0(h) = (p2/3 + p1/2 + p0) * h\n",
    "            C0h[:, 1:] = (P2/3 + P1/2 + P0) * h\n",
    "            C0h = torch.cumsum(C0h, dim=1)  # CDF on grid\n",
    "            Zc  = C0h[:,-1].reshape(-1,1)\n",
    "            i0 = (Zc<1e-30).flatten()\n",
    "            P2[i0,:] = 0\n",
    "            P1[i0,:] = 0\n",
    "            P0[i0,:] = 1\n",
    "            C0h[i0, 1:] = h\n",
    "            C0h[i0, :] = torch.cumsum(C0h[i0, :], dim=1)  # CDF on grid\n",
    "            Zc  = C0h[:,-1].reshape(-1,1)\n",
    "            C0h = C0h / Zc  # Normalise CDF\n",
    "            ind = torch.searchsorted(C0h, q[:,k].reshape(-1,1)) - 1 # indices on the left of q\n",
    "            ind = torch.minimum(torch.maximum(ind, torch.tensor(0, device=self.U[0].device)), torch.tensor(self.grid[k].numel()-2, device=self.U[0].device))\n",
    "            P0 = torch.gather(P0, dim=1, index=ind) / Zc\n",
    "            P1 = torch.gather(P1, dim=1, index=ind) / Zc\n",
    "            P2 = torch.gather(P2, dim=1, index=ind) / Zc\n",
    "            xl = self.grid[k][ind].reshape(-1,1)\n",
    "            h = self.grid[k][ind+1].reshape(-1,1) - xl\n",
    "            q0 = q[:,k].reshape(-1,1) - torch.gather(C0h, dim=1, index=ind)\n",
    "            # Linear initial guess\n",
    "            z = q0 / (torch.gather(C0h, dim=1, index=ind+1) - torch.gather(C0h, dim=1, index=ind))\n",
    "            # Where equation is cubic\n",
    "            i3 = (P2 > 1e-30).flatten()\n",
    "            P0v = P0[i3]\n",
    "            P1v = P1[i3]\n",
    "            P2v = P2[i3]\n",
    "            D0 = P1v**2/4 - P0v*P2v\n",
    "            D1 = P1v**3/4 - P0v*P1v*P2v*1.5 - 3*P2v**2*q0[i3]/h[i3]\n",
    "            DD = D1**2 - 4*D0**3\n",
    "            C = (D1 + D1.sign() * torch.sqrt(DD.abs()))/2\n",
    "            C = C.sign() * C.abs().pow(1.0/3)\n",
    "            D0C = D0/C\n",
    "            D0C[C.abs() < 1e-30] = 0\n",
    "            C[C.abs() < 1e-30] = 0\n",
    "            z[i3] = -(P1v/2 + C + D0C)/P2v\n",
    "            # Where equation is quadratic\n",
    "            i2 = ((P2 < 1e-30) & (P1.abs() > 1e-30)).flatten()\n",
    "            P0v = P0[i2]\n",
    "            P1v = P1[i2]\n",
    "            DD = P0v**2 + 2*P1v*q0[i2]/h[i2]\n",
    "            z[i2] = (-P0v + torch.sqrt(DD.abs()))/P1v\n",
    "            # z = torch.min(torch.max(z, torch.zeros_like(z)), torch.ones_like(z))\n",
    "            # res = (P2/3 * z**3 + P1/2 * z**2 + P0 * z)*h - q0\n",
    "            # print(f\"root residual={torch.sqrt(torch.mean(res**2))}, {torch.sum(i3)} cubic, {torch.sum(i2)} quadratic, {torch.sum(~i2 & ~i3)} linear\")\n",
    "            x[:,k] = (xl + z*h).flatten()\n",
    "            lFapp += torch.log(P2 * z**2 + P1 * z + P0)\n",
    "            prevcond = torch.einsum('mk,kml->ml', prevcond, self.interp_core(k, x[:,k]))\n",
    "        return x, lFapp  \n",
    "    \n",
    "    def irt2(self, qn):\n",
    "        \"\"\"\n",
    "        Inverse Rosenblatt transform linearly interpolating squared nodal values of TT\n",
    "        \"\"\"\n",
    "        q = qn\n",
    "        if self.reference_sigma > 0:\n",
    "            # Map from normal reference to uniform\n",
    "            q = torch.erf(qn/torch.sqrt(torch.tensor(2.0, dtype=self.U[0].dtype))) * self.cdf_factor + 0.5  \n",
    "        self.normconst()\n",
    "        prevcond = torch.ones(q.shape[0],1, device=self.U[0].device, dtype=self.U[0].dtype) # Conditioned left cores\n",
    "        lFapp = torch.zeros_like(prevcond)\n",
    "        x = torch.ones_like(q)        \n",
    "        for k in range(len(self.U)):\n",
    "            Pprev = torch.einsum('mk,kil->mil', prevcond, self.U[k])\n",
    "            D, C = torch.linalg.eigh(self.marginals[k+1])\n",
    "            D[D<0] = 0\n",
    "            D = torch.sqrt(D)\n",
    "            C = C @ torch.diag(D)\n",
    "            # C, info = torch.linalg.cholesky_ex(self.marginals[k+1], upper=False)\n",
    "            P = torch.einsum('mik,kp->mip', Pprev, C)\n",
    "            P = P**2\n",
    "            P = P.sum(dim=2)\n",
    "            # P2 = torch.einsum('mil,mip,lp->mi', Pprev, Pprev, self.marginals[k+1])\n",
    "            C0h = torch.zeros_like(P)\n",
    "            h = (self.grid[k][1:] - self.grid[k][:-1]).reshape(1,-1)\n",
    "            C0h[:, 1:] = 0.5 * (P[:,1:] + P[:,:-1]) * h\n",
    "            C0h = torch.cumsum(C0h, dim=1)  # CDF on grid\n",
    "            Zc  = C0h[:,-1].reshape(-1,1)\n",
    "            i0 = (Zc<1e-30).flatten()\n",
    "            P[i0, :] = 1\n",
    "            C0h[i0, 1:] = h\n",
    "            C0h[i0, :] = torch.cumsum(C0h[i0, :], dim=1)  # CDF on grid\n",
    "            Zc  = C0h[:,-1].reshape(-1,1)\n",
    "            C0h = C0h / Zc  # Normalise CDF\n",
    "            P = P / Zc\n",
    "            ind = torch.searchsorted(C0h, q[:,k].reshape(-1,1)) - 1 # indices on the left of q\n",
    "            ind = torch.minimum(torch.maximum(ind, torch.tensor(0, device=self.U[0].device)), torch.tensor(self.grid[k].numel()-2, device=self.U[0].device))\n",
    "            P1 = torch.gather(P, dim=1, index=ind)\n",
    "            P2 = torch.gather(P, dim=1, index=ind+1)\n",
    "            xl = self.grid[k][ind].reshape(-1,1)\n",
    "            h = self.grid[k][ind+1].reshape(-1,1) - xl\n",
    "            q0 = q[:,k].reshape(-1,1) - torch.gather(C0h, dim=1, index=ind)\n",
    "            # Linear initial guess\n",
    "            z = q0 / (torch.gather(C0h, dim=1, index=ind+1) - torch.gather(C0h, dim=1, index=ind))\n",
    "            # Where equation is quadratic\n",
    "            Aq = 0.5 * (P2 - P1) / h\n",
    "            i2 = (Aq.abs() > 1e-30).flatten()\n",
    "            Dq = P1[i2]**2 + 4 * Aq[i2] * q0[i2]\n",
    "            z[i2] = (-P1[i2] + torch.sqrt(torch.abs(Dq))) / (2*Aq[i2]*h[i2])\n",
    "            # z = torch.min(torch.max(z, torch.zeros_like(z)), torch.ones_like(z))\n",
    "            x[:,k] = (xl + z*h).flatten()\n",
    "            lFapp += torch.log(P1 + (P2 - P1) * z)\n",
    "            prevcond = torch.einsum('mk,kml->ml', prevcond, self.interp_core(k, x[:,k]))\n",
    "        return x, lFapp  \n",
    "\n",
    "\n",
    "\n",
    "class DIRT(nn.Module):\n",
    "    def __init__(self, grid=[torch.ones(1)], r=[1,1], beta=[1], ExactRatio=True, Ntest=1000, reference_sigma=0, requires_grad=True, use_irt2=False):\n",
    "        super().__init__()\n",
    "        if reference_sigma > 0:\n",
    "            grid1 = [torch.linspace(-reference_sigma, reference_sigma, g.numel(), device=g.device, dtype=g.dtype) for g in grid]\n",
    "        else:\n",
    "            grid1 = [0.5*(torch.cos(torch.pi*torch.arange(g.numel()-1, -1, -1, device=g.device, dtype=g.dtype)/(g.numel()-1))+1) for g in grid]\n",
    "        self.Densities = [TTSqrDE(grid1, r, reference_sigma=reference_sigma, requires_grad=requires_grad) for _ in range(len(beta))]\n",
    "        self.Densities[0] = TTSqrDE(grid, r, reference_sigma=reference_sigma, requires_grad=requires_grad)\n",
    "        self.beta = beta\n",
    "        self.Ntest = Ntest\n",
    "        self.ExactRatio = ExactRatio\n",
    "        self.use_irt2 = use_irt2\n",
    "        self.CurrentLayer = -1\n",
    "        self.lFshift = 0\n",
    "\n",
    "    def forward(self, q):\n",
    "        x = q.clone()\n",
    "        lFapp = torch.zeros_like(q[:,0])\n",
    "        for L in range(self.CurrentLayer, -1, -1):\n",
    "            if self.use_irt2:\n",
    "                x, dlFapp = self.Densities[L].irt2(x)\n",
    "            else:\n",
    "                x, dlFapp = self.Densities[L].irt(x)\n",
    "            lFapp += dlFapp.flatten()\n",
    "            if (L > 0) and (self.Densities[L-1].reference_sigma > 0):\n",
    "                lFapp += 0.5*torch.sum(x**2, dim=1) - torch.log(2*self.Densities[L-1].cdf_factor**2/torch.pi)*x.shape[1]/2\n",
    "        return x, lFapp\n",
    "    \n",
    "    def ratio_function(self, logdensity, q, beta_high, beta_low):\n",
    "        x, lFapp = self.forward(q)\n",
    "        if self.ExactRatio:\n",
    "            F = logdensity(x, beta_high, 0) - lFapp - self.lFshift\n",
    "        else:\n",
    "            F = logdensity(x, beta_high, beta_low) - self.lFshift\n",
    "        if (self.Densities[0].reference_sigma > 0) and (beta_low > 0):\n",
    "            F += -0.5*torch.sum(q**2, dim=1) + torch.log(2*self.Densities[0].cdf_factor**2/torch.pi)*q.shape[1]/2\n",
    "        return torch.exp(0.5*F)\n",
    "    \n",
    "    def initial_guess(self, L):\n",
    "        if L>0:\n",
    "            for k in range(len(self.Densities[L].U)):\n",
    "                self.Densities[L].U[k].data = self.Densities[L-1].U[k].data.clone()\n",
    "\n",
    "    def cross(self, logdensity, tol=1e-6, nswp=20, dir=1):\n",
    "        \"\"\"\n",
    "        DIRT with cross approximations of layers\n",
    "        logdensity should take samples of shape (M, d), beta_high, beta_low, and return a tensor of shape (M)\n",
    "        \"\"\"\n",
    "        evalcnt = 0\n",
    "        beta = torch.cat([torch.zeros(1), self.beta])\n",
    "        for L in range(len(beta)-1):\n",
    "            print(f\"Approximating level {L}, for beta={beta[L+1]}\")\n",
    "            self.initial_guess(L)\n",
    "            devalcnt = self.Densities[L].cross(lambda x: self.ratio_function(logdensity, x, beta[L+1], beta[L]), tol=tol, nswp=nswp, dir=dir)\n",
    "            evalcnt += devalcnt\n",
    "            self.CurrentLayer = L\n",
    "            if self.Ntest > 0:\n",
    "                qtest = randref(self.Densities[L].reference_sigma, (self.Ntest, len(self.Densities[0].grid)), device=self.Densities[0].U[0].device, dtype=self.Densities[0].U[0].dtype)\n",
    "                z, lFapp = self.forward(qtest)\n",
    "                lFex = logdensity(z, beta[L+1], 0)\n",
    "                # plt.scatter(z[:,0].cpu().detach().numpy(), z[:,1].cpu().detach().numpy(), c=torch.exp(lFex).cpu().detach().numpy(), s=1)\n",
    "                # plt.title(f\"Level {L}\")\n",
    "                # plt.show()\n",
    "                print(f\"Hellinger distance = {hellinger(lFex, lFapp):.3e}\")\n",
    "                if L < len(beta)-2:\n",
    "                    if self.ExactRatio:\n",
    "                        self.lFshift = torch.max(lFex)*beta[L+2]/beta[L+1] - torch.max(lFapp)\n",
    "                    else:\n",
    "                        self.lFshift = torch.max(lFex)*(beta[L+2] - beta[L+1])/beta[L+1]\n",
    "        return evalcnt         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DIRT([torch.arange(2, dtype=torch.float32)], [1,1])\n",
    "# t = TTSqrDE([torch.tensor([1,3,4,5,7,9], dtype=torch.float32)], [1,1])\n",
    "# t.U[0].data = t.U[0].data * 0 # torch.exp(-t.grid[0]**2/2).reshape(1,-1,1)\n",
    "# t.irt2(torch.linspace(0,1,7).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0206, 0.7502, 5.4087], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(3,3, dtype=torch.float64, device='cpu')\n",
    "print(torch.linalg.eigh(A.T @ A)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10665/1820986772.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  A = torch.tensor(A, dtype=torch.float32, device='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4598,  1.3859,  0.3691],\n",
       "        [ 0.4487, -0.1023, -0.6304],\n",
       "        [ 0.7000,  1.6681, -0.1714]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor(A, dtype=torch.float32, device='cpu')\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximating level 0, for beta=0.015625\n",
      "\t swp = 0.0: err = 1.000e+00, #evals = 231\n",
      "\t swp = 0.1: err = 5.826e-01, #evals = 1617\n",
      "Cross sweep 0: max_err = 1.000e+00, cum#evals = 1848\n",
      "\t swp = 0.2: err = 6.707e-01, #evals = 231\n",
      "Hellinger distance = 1.581e-03\n",
      "Approximating level 1, for beta=0.0625\n",
      "\t swp = 0.0: err = 8.586e-01, #evals = 231\n",
      "\t swp = 0.1: err = 3.885e-01, #evals = 1617\n",
      "Cross sweep 0: max_err = 8.586e-01, cum#evals = 1848\n",
      "\t swp = 0.2: err = 4.120e-03, #evals = 231\n",
      "Hellinger distance = 1.711e-03\n",
      "Approximating level 2, for beta=0.25\n",
      "\t swp = 0.0: err = 4.903e-01, #evals = 231\n",
      "\t swp = 0.1: err = 7.633e-02, #evals = 1617\n",
      "Cross sweep 0: max_err = 4.903e-01, cum#evals = 1848\n",
      "\t swp = 0.2: err = 2.213e-04, #evals = 231\n",
      "Hellinger distance = 3.122e-03\n",
      "Approximating level 3, for beta=1.0\n",
      "\t swp = 0.0: err = 5.233e-01, #evals = 231\n",
      "\t swp = 0.1: err = 1.907e-02, #evals = 1617\n",
      "Cross sweep 0: max_err = 5.233e-01, cum#evals = 1848\n",
      "\t swp = 0.2: err = 6.378e-04, #evals = 231\n",
      "Hellinger distance = 3.439e-03\n",
      "Approximating level 4, for beta=4.0\n",
      "\t swp = 0.0: err = 6.273e-01, #evals = 231\n",
      "\t swp = 0.1: err = 1.214e-03, #evals = 1617\n",
      "Cross sweep 0: max_err = 6.273e-01, cum#evals = 1848\n",
      "\t swp = 0.2: err = 4.151e-04, #evals = 231\n",
      "Hellinger distance = 4.949e-03\n",
      "Approximating level 5, for beta=16.0\n",
      "\t swp = 0.0: err = 2.688e-01, #evals = 231\n",
      "\t swp = 0.1: err = 1.078e-03, #evals = 1617\n",
      "Cross sweep 0: max_err = 2.688e-01, cum#evals = 1848\n",
      "\t swp = 0.2: err = 6.295e-04, #evals = 231\n",
      "Hellinger distance = 3.846e-03\n",
      "Approximating level 6, for beta=64.0\n",
      "\t swp = 0.0: err = 1.717e-02, #evals = 231\n",
      "\t swp = 0.1: err = 2.133e-03, #evals = 1617\n",
      "Cross sweep 0: max_err = 1.717e-02, cum#evals = 1848\n",
      "\t swp = 0.2: err = 7.977e-04, #evals = 231\n",
      "Hellinger distance = 4.409e-03\n",
      "total evals = 14553, time = 0.4799535274505615\n"
     ]
    }
   ],
   "source": [
    "# Approximate\n",
    "t = DIRT([torch.linspace(-7,7,33, dtype=torch.float32, device='cpu')]*3, [1,7,7,1], 4.0**torch.arange(-3,4), \\\n",
    "          ExactRatio=False, Ntest=10**3, reference_sigma=0.0, requires_grad=False, use_irt2=True)\n",
    "costf = lambda x: -0.5*torch.sum((x @ A)**2, dim=1)\n",
    "tempered_costf = lambda x, beta_high, beta_low: costf(x)*(beta_high - beta_low)\n",
    "t1 = time.time()\n",
    "ev = t.cross(tempered_costf, nswp=1)\n",
    "t2 = time.time()\n",
    "print(f\"total evals = {ev}, time = {t2-t1}\")\n",
    "# x = torch.rand(10**5,3, dtype=t.Densities[0].U[0].dtype, device=t.Densities[0].U[0].device)*14-7\n",
    "# plt.scatter(x[:,0].cpu().detach().numpy(), x[:,1].cpu().detach().numpy(), 1, torch.exp(0.5*(4.0**-4)*costf(x)).cpu().detach().numpy())\n",
    "# plt.show()\n",
    "# print(\"error0\", torch.mean((torch.sqrt(t.Densities[0].forward(x)).flatten() - torch.exp(0.5*(4.0**-3)*costf(x)))**2) / torch.mean(torch.exp(0.5*(4.0**-3)*costf(x))**2))\n",
    "\n",
    "# # Sample\n",
    "# x = randref(t.Densities[0].reference_sigma, (10**4,3), device=t.Densities[0].U[0].device, dtype=t.Densities[0].U[0].dtype)\n",
    "# z, lFapp = t.forward(x)\n",
    "# lFex = tempered_costf(z, 4.0**3, 0)\n",
    "# z, lFex, lFapp, rej = mcmc_prune(z, lFex, lFapp)\n",
    "\n",
    "# plt.scatter(z[:,0].cpu().detach().numpy(), z[:,1].cpu().detach().numpy(), 1, torch.exp(lFex).cpu().detach().numpy())\n",
    "# plt.colorbar()\n",
    "# t.CurrentLayer\n",
    "# X, Y = torch.meshgrid(torch.linspace(-3,3,99), torch.linspace(-3,3,99))\n",
    "# plt.contour(X,Y,t.Density0.forward(torch.hstack([X.reshape(-1,1), Y.reshape(-1,1)])).reshape(99,99).detach().numpy(), levels=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t swp = 0.0: err = 1.000e+00, #evals = 2000\n",
      "\t swp = 0.1: err = 2.398e-01, #evals = 80000\n",
      "Cross sweep 0: max_err = 1.000e+00, cum#evals = 82000\n",
      "\t swp = 1.2: err = 1.428e-01, #evals = 2000\n",
      "\t swp = 1.1: err = 5.578e-07, #evals = 80000\n",
      "Cross sweep 1: max_err = 1.428e-01, cum#evals = 164000\n",
      "\t swp = 2.0: err = 1.321e-05, #evals = 2000\n",
      "\t swp = 2.1: err = 5.910e-07, #evals = 80000\n",
      "Cross sweep 2: max_err = 1.321e-05, cum#evals = 246000\n",
      "\t swp = 3.2: err = 1.848e-06, #evals = 2000\n",
      "\t swp = 3.1: err = 8.628e-07, #evals = 80000\n",
      "Cross sweep 3: max_err = 1.848e-06, cum#evals = 328000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dc/.local/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/tmp/ipykernel_18317/1125787919.py:13: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  ind = torch.searchsorted(grid, x) # indices on the right of x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# def ff(x):\n",
    "#     return np.sqrt(np.sum(x+1, axis=1, keepdims=True))\n",
    "\n",
    "# def torchnumpy(fun, x):\n",
    "#     return torch.tensor(fun(x.detach().numpy()))\n",
    "\n",
    "t = TTSqrDE([torch.linspace(0,49,50)]*3, [1]+2*[40]+[1])\n",
    "t.cross(lambda x: torch.sqrt(torch.sum(x+1, axis=1, keepdims=True)), tol=1e-5, nswp=33)\n",
    "\n",
    "X1, X2, X3 = torch.meshgrid(torch.linspace(0,9,10), torch.linspace(0,9,10), torch.linspace(0,9,10))\n",
    "torch.norm( (t.forward(torch.hstack([X1.reshape(-1,1), X2.reshape(-1,1), X3.reshape(-1,1)]))**(1/2)).reshape(X1.shape) - torch.sqrt(X1+X2+X3+3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0356,  1.3452,  3.3218], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0356, dtype=torch.float64)"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(3,3, dtype=torch.float64, device='cpu')\n",
    "A = A + A.T\n",
    "L = torch.linalg.eigh(A)[0]\n",
    "A = A + 3*torch.eye(3, dtype=torch.float64, device='cpu')\n",
    "# LD, piv = torch.linalg.ldl_factor(A, hermitian=True)\n",
    "# ind = torch.arange(A.shape[0], device=A.device)\n",
    "# for i,p in enumerate(piv-1): # Swap rows\n",
    "#     ind[i], ind[p] = ind[p]+0, ind[i]+0    \n",
    "# C = torch.tril(LD, diagonal=-1)\n",
    "# C = C[ind, :]\n",
    "# D = torch.diag(LD)\n",
    "# D[D<0] = 0\n",
    "# D = torch.sqrt(D)\n",
    "# C = C + torch.eye(D.numel(), dtype=D.dtype, device=D.device)\n",
    "# C = C @ torch.diag(D)\n",
    "# print(ind)\n",
    "# print(D)\n",
    "# print(C)\n",
    "# (A - C @ C.T).norm()\n",
    "D, C = torch.linalg.eigh(A)\n",
    "print(D)\n",
    "D[D<0] = 0\n",
    "D = torch.sqrt(D)\n",
    "C = C @ torch.diag(D)\n",
    "(A - C @ C.T).norm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
